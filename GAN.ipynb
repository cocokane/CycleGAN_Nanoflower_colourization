{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200] Batch [0] Loss G: 12.94000244140625 Loss D_A: 0.4972791075706482 Loss D_B: 0.6664077639579773\n",
      "Epoch [0/200] Batch [100] Loss G: 7.225565433502197 Loss D_A: 0.204205721616745 Loss D_B: 0.21398532390594482\n",
      "Epoch [0/200] Batch [200] Loss G: 6.966743469238281 Loss D_A: 0.21361631155014038 Loss D_B: 0.18268409371376038\n",
      "Epoch [0/200] Batch [300] Loss G: 5.600608825683594 Loss D_A: 0.21613283455371857 Loss D_B: 0.49126988649368286\n",
      "Epoch [0/200] Batch [400] Loss G: 5.61020040512085 Loss D_A: 0.19709672033786774 Loss D_B: 0.1868860125541687\n",
      "Epoch [1/200] Batch [0] Loss G: 3.9963855743408203 Loss D_A: 0.1904582679271698 Loss D_B: 0.24450989067554474\n",
      "Epoch [1/200] Batch [100] Loss G: 4.690371036529541 Loss D_A: 0.2334848940372467 Loss D_B: 0.18060357868671417\n",
      "Epoch [1/200] Batch [200] Loss G: 4.797698020935059 Loss D_A: 0.2057403326034546 Loss D_B: 0.16414861381053925\n",
      "Epoch [1/200] Batch [300] Loss G: 4.4426703453063965 Loss D_A: 0.1968402862548828 Loss D_B: 0.16190281510353088\n",
      "Epoch [1/200] Batch [400] Loss G: 5.28502082824707 Loss D_A: 0.22705942392349243 Loss D_B: 0.16446399688720703\n",
      "Epoch [2/200] Batch [0] Loss G: 4.786965847015381 Loss D_A: 0.17024630308151245 Loss D_B: 0.3411789834499359\n",
      "Epoch [2/200] Batch [100] Loss G: 4.482774257659912 Loss D_A: 0.1991802453994751 Loss D_B: 0.3006528913974762\n",
      "Epoch [2/200] Batch [200] Loss G: 3.6509389877319336 Loss D_A: 0.20861437916755676 Loss D_B: 0.18599051237106323\n",
      "Epoch [2/200] Batch [300] Loss G: 3.546051263809204 Loss D_A: 0.18667517602443695 Loss D_B: 0.20267316699028015\n",
      "Epoch [2/200] Batch [400] Loss G: 4.366796970367432 Loss D_A: 0.15627776086330414 Loss D_B: 0.1451035887002945\n",
      "Epoch [3/200] Batch [0] Loss G: 4.107668399810791 Loss D_A: 0.23765169084072113 Loss D_B: 0.2607177495956421\n",
      "Epoch [3/200] Batch [100] Loss G: 3.4397835731506348 Loss D_A: 0.13882339000701904 Loss D_B: 0.2809045910835266\n",
      "Epoch [3/200] Batch [200] Loss G: 5.624580383300781 Loss D_A: 0.42970603704452515 Loss D_B: 0.15900304913520813\n",
      "Epoch [3/200] Batch [300] Loss G: 4.313126087188721 Loss D_A: 0.15608802437782288 Loss D_B: 0.24614834785461426\n",
      "Epoch [3/200] Batch [400] Loss G: 3.712550401687622 Loss D_A: 0.12944912910461426 Loss D_B: 0.1513182520866394\n",
      "Epoch [4/200] Batch [0] Loss G: 3.608100175857544 Loss D_A: 0.20636224746704102 Loss D_B: 0.21579231321811676\n",
      "Epoch [4/200] Batch [100] Loss G: 3.448517322540283 Loss D_A: 0.22468581795692444 Loss D_B: 0.25038009881973267\n",
      "Epoch [4/200] Batch [200] Loss G: 3.9519834518432617 Loss D_A: 0.16618822515010834 Loss D_B: 0.22146983444690704\n",
      "Epoch [4/200] Batch [300] Loss G: 3.888498306274414 Loss D_A: 0.15302017331123352 Loss D_B: 0.21828345954418182\n",
      "Epoch [4/200] Batch [400] Loss G: 3.6580686569213867 Loss D_A: 0.17332912981510162 Loss D_B: 0.11131352186203003\n",
      "Epoch [5/200] Batch [0] Loss G: 5.109920978546143 Loss D_A: 0.2781086266040802 Loss D_B: 0.15449556708335876\n",
      "Epoch [5/200] Batch [100] Loss G: 3.637369155883789 Loss D_A: 0.14239248633384705 Loss D_B: 0.1577320694923401\n",
      "Epoch [5/200] Batch [200] Loss G: 3.214137554168701 Loss D_A: 0.3093014657497406 Loss D_B: 0.15995357930660248\n",
      "Epoch [5/200] Batch [300] Loss G: 4.775545120239258 Loss D_A: 0.11215238273143768 Loss D_B: 0.16951987147331238\n",
      "Epoch [5/200] Batch [400] Loss G: 3.525277614593506 Loss D_A: 0.08700641244649887 Loss D_B: 0.22152352333068848\n",
      "Epoch [6/200] Batch [0] Loss G: 2.994791269302368 Loss D_A: 0.21420495212078094 Loss D_B: 0.22128655016422272\n",
      "Epoch [6/200] Batch [100] Loss G: 3.2956371307373047 Loss D_A: 0.23667335510253906 Loss D_B: 0.3128584921360016\n",
      "Epoch [6/200] Batch [200] Loss G: 3.01627779006958 Loss D_A: 0.18737170100212097 Loss D_B: 0.15384915471076965\n",
      "Epoch [6/200] Batch [300] Loss G: 3.341094493865967 Loss D_A: 0.19506676495075226 Loss D_B: 0.16188925504684448\n",
      "Epoch [6/200] Batch [400] Loss G: 4.356947422027588 Loss D_A: 0.25690993666648865 Loss D_B: 0.18460652232170105\n",
      "Epoch [7/200] Batch [0] Loss G: 3.7686891555786133 Loss D_A: 0.20135003328323364 Loss D_B: 0.17760254442691803\n",
      "Epoch [7/200] Batch [100] Loss G: 3.413663387298584 Loss D_A: 0.1470695286989212 Loss D_B: 0.24868859350681305\n",
      "Epoch [7/200] Batch [200] Loss G: 3.299372673034668 Loss D_A: 0.19771167635917664 Loss D_B: 0.17984424531459808\n",
      "Epoch [7/200] Batch [300] Loss G: 3.656609058380127 Loss D_A: 0.11101774126291275 Loss D_B: 0.228310227394104\n",
      "Epoch [7/200] Batch [400] Loss G: 4.14129114151001 Loss D_A: 0.20594650506973267 Loss D_B: 0.2527422308921814\n",
      "Epoch [8/200] Batch [0] Loss G: 2.8204336166381836 Loss D_A: 0.25113964080810547 Loss D_B: 0.1339857429265976\n",
      "Epoch [8/200] Batch [100] Loss G: 3.3067541122436523 Loss D_A: 0.2655698359012604 Loss D_B: 0.21011008322238922\n",
      "Epoch [8/200] Batch [200] Loss G: 3.2399139404296875 Loss D_A: 0.24327042698860168 Loss D_B: 0.0973726287484169\n",
      "Epoch [8/200] Batch [300] Loss G: 2.795020818710327 Loss D_A: 0.24965912103652954 Loss D_B: 0.20392541587352753\n",
      "Epoch [8/200] Batch [400] Loss G: 2.8056447505950928 Loss D_A: 0.2367933690547943 Loss D_B: 0.1947632133960724\n",
      "Epoch [9/200] Batch [0] Loss G: 2.806788921356201 Loss D_A: 0.23043569922447205 Loss D_B: 0.2651175856590271\n",
      "Epoch [9/200] Batch [100] Loss G: 2.6652991771698 Loss D_A: 0.22239628434181213 Loss D_B: 0.16025730967521667\n",
      "Epoch [9/200] Batch [200] Loss G: 2.779606342315674 Loss D_A: 0.21917584538459778 Loss D_B: 0.1707225739955902\n",
      "Epoch [9/200] Batch [300] Loss G: 3.1664881706237793 Loss D_A: 0.2400725781917572 Loss D_B: 0.20068688690662384\n",
      "Epoch [9/200] Batch [400] Loss G: 2.478278636932373 Loss D_A: 0.1943976879119873 Loss D_B: 0.18154266476631165\n",
      "Epoch [10/200] Batch [0] Loss G: 2.8313074111938477 Loss D_A: 0.2532515227794647 Loss D_B: 0.2748432159423828\n",
      "Epoch [10/200] Batch [100] Loss G: 3.7643260955810547 Loss D_A: 0.342841237783432 Loss D_B: 0.1974124163389206\n",
      "Epoch [10/200] Batch [200] Loss G: 3.3214807510375977 Loss D_A: 0.16894593834877014 Loss D_B: 0.20958654582500458\n",
      "Epoch [10/200] Batch [300] Loss G: 3.140878438949585 Loss D_A: 0.13967014849185944 Loss D_B: 0.15163001418113708\n",
      "Epoch [10/200] Batch [400] Loss G: 3.8817543983459473 Loss D_A: 0.03617377579212189 Loss D_B: 0.2558605372905731\n",
      "Epoch [11/200] Batch [0] Loss G: 2.9864020347595215 Loss D_A: 0.18327611684799194 Loss D_B: 0.1328381896018982\n",
      "Epoch [11/200] Batch [100] Loss G: 3.4309682846069336 Loss D_A: 0.10433539748191833 Loss D_B: 0.16789954900741577\n",
      "Epoch [11/200] Batch [200] Loss G: 3.528014659881592 Loss D_A: 0.12449430674314499 Loss D_B: 0.23783716559410095\n",
      "Epoch [11/200] Batch [300] Loss G: 2.907334566116333 Loss D_A: 0.13991332054138184 Loss D_B: 0.25974857807159424\n",
      "Epoch [11/200] Batch [400] Loss G: 2.729804754257202 Loss D_A: 0.19300848245620728 Loss D_B: 0.21247008442878723\n",
      "Epoch [12/200] Batch [0] Loss G: 3.303328514099121 Loss D_A: 0.19575385749340057 Loss D_B: 0.2596445381641388\n",
      "Epoch [12/200] Batch [100] Loss G: 3.0248665809631348 Loss D_A: 0.1638120412826538 Loss D_B: 0.18646281957626343\n",
      "Epoch [12/200] Batch [200] Loss G: 2.9444870948791504 Loss D_A: 0.13135385513305664 Loss D_B: 0.22065597772598267\n",
      "Epoch [12/200] Batch [300] Loss G: 2.5847294330596924 Loss D_A: 0.46456319093704224 Loss D_B: 0.21564200520515442\n",
      "Epoch [12/200] Batch [400] Loss G: 3.2589621543884277 Loss D_A: 0.02233291231095791 Loss D_B: 0.23174914717674255\n",
      "Epoch [13/200] Batch [0] Loss G: 2.3088338375091553 Loss D_A: 0.29037782549858093 Loss D_B: 0.1825961470603943\n",
      "Epoch [13/200] Batch [100] Loss G: 3.0343761444091797 Loss D_A: 0.18171346187591553 Loss D_B: 0.1972103714942932\n",
      "Epoch [13/200] Batch [200] Loss G: 2.646927833557129 Loss D_A: 0.221914142370224 Loss D_B: 0.2214825600385666\n",
      "Epoch [13/200] Batch [300] Loss G: 2.770270824432373 Loss D_A: 0.09040713310241699 Loss D_B: 0.25212615728378296\n",
      "Epoch [13/200] Batch [400] Loss G: 3.497107982635498 Loss D_A: 0.4388117790222168 Loss D_B: 0.1971973180770874\n",
      "Epoch [14/200] Batch [0] Loss G: 2.6655983924865723 Loss D_A: 0.17125621438026428 Loss D_B: 0.2932206988334656\n",
      "Epoch [14/200] Batch [100] Loss G: 2.8305675983428955 Loss D_A: 0.21638748049736023 Loss D_B: 0.22412234544754028\n",
      "Epoch [14/200] Batch [200] Loss G: 2.966108798980713 Loss D_A: 0.18692471086978912 Loss D_B: 0.151108980178833\n",
      "Epoch [14/200] Batch [300] Loss G: 2.8111252784729004 Loss D_A: 0.38477638363838196 Loss D_B: 0.20038913190364838\n",
      "Epoch [14/200] Batch [400] Loss G: 2.9927289485931396 Loss D_A: 0.23231300711631775 Loss D_B: 0.16558872163295746\n",
      "Epoch [15/200] Batch [0] Loss G: 2.9756481647491455 Loss D_A: 0.24740467965602875 Loss D_B: 0.2478465437889099\n",
      "Epoch [15/200] Batch [100] Loss G: 3.3474347591400146 Loss D_A: 0.14625895023345947 Loss D_B: 0.16708728671073914\n",
      "Epoch [15/200] Batch [200] Loss G: 2.983102321624756 Loss D_A: 0.14273692667484283 Loss D_B: 0.2057144045829773\n",
      "Epoch [15/200] Batch [300] Loss G: 2.2257730960845947 Loss D_A: 0.2602357864379883 Loss D_B: 0.29343870282173157\n",
      "Epoch [15/200] Batch [400] Loss G: 2.7750813961029053 Loss D_A: 0.23147615790367126 Loss D_B: 0.17475353181362152\n",
      "Epoch [16/200] Batch [0] Loss G: 2.8000078201293945 Loss D_A: 0.14749287068843842 Loss D_B: 0.1919948011636734\n",
      "Epoch [16/200] Batch [100] Loss G: 2.5142874717712402 Loss D_A: 0.09065307676792145 Loss D_B: 0.27644312381744385\n",
      "Epoch [16/200] Batch [200] Loss G: 3.0195062160491943 Loss D_A: 0.11693073809146881 Loss D_B: 0.2968601584434509\n",
      "Epoch [16/200] Batch [300] Loss G: 2.6944637298583984 Loss D_A: 0.2117718756198883 Loss D_B: 0.2260122001171112\n",
      "Epoch [16/200] Batch [400] Loss G: 2.463012218475342 Loss D_A: 0.1706787347793579 Loss D_B: 0.24309484660625458\n",
      "Epoch [17/200] Batch [0] Loss G: 2.8206608295440674 Loss D_A: 0.1632964164018631 Loss D_B: 0.2611519694328308\n",
      "Epoch [17/200] Batch [100] Loss G: 3.664757251739502 Loss D_A: 0.14295175671577454 Loss D_B: 0.22083650529384613\n",
      "Epoch [17/200] Batch [200] Loss G: 2.990994930267334 Loss D_A: 0.16802653670310974 Loss D_B: 0.19403286278247833\n",
      "Epoch [17/200] Batch [300] Loss G: 2.618556499481201 Loss D_A: 0.16022442281246185 Loss D_B: 0.21443992853164673\n",
      "Epoch [17/200] Batch [400] Loss G: 2.667288303375244 Loss D_A: 0.1933194398880005 Loss D_B: 0.19244813919067383\n",
      "Epoch [18/200] Batch [0] Loss G: 2.8827600479125977 Loss D_A: 0.39468520879745483 Loss D_B: 0.23643779754638672\n",
      "Epoch [18/200] Batch [100] Loss G: 3.2965004444122314 Loss D_A: 0.23334480822086334 Loss D_B: 0.19749853014945984\n",
      "Epoch [18/200] Batch [200] Loss G: 3.4065675735473633 Loss D_A: 0.10317116230726242 Loss D_B: 0.20417101681232452\n",
      "Epoch [18/200] Batch [300] Loss G: 3.1644835472106934 Loss D_A: 0.1841932088136673 Loss D_B: 0.282081663608551\n",
      "Epoch [18/200] Batch [400] Loss G: 3.450120687484741 Loss D_A: 0.29790180921554565 Loss D_B: 0.3115323781967163\n",
      "Epoch [19/200] Batch [0] Loss G: 3.064925193786621 Loss D_A: 0.06405952572822571 Loss D_B: 0.27277740836143494\n",
      "Epoch [19/200] Batch [100] Loss G: 2.9584498405456543 Loss D_A: 0.08723293244838715 Loss D_B: 0.2323182225227356\n",
      "Epoch [19/200] Batch [200] Loss G: 3.742264747619629 Loss D_A: 0.1729857325553894 Loss D_B: 0.172251895070076\n",
      "Epoch [19/200] Batch [300] Loss G: 2.678894519805908 Loss D_A: 0.2882796823978424 Loss D_B: 0.22941070795059204\n",
      "Epoch [19/200] Batch [400] Loss G: 2.424600124359131 Loss D_A: 0.19021114706993103 Loss D_B: 0.29442358016967773\n",
      "Epoch [20/200] Batch [0] Loss G: 3.3563899993896484 Loss D_A: 0.19105805456638336 Loss D_B: 0.17810848355293274\n",
      "Epoch [20/200] Batch [100] Loss G: 3.608673334121704 Loss D_A: 0.331181138753891 Loss D_B: 0.15424135327339172\n",
      "Epoch [20/200] Batch [200] Loss G: 2.4330615997314453 Loss D_A: 0.19612230360507965 Loss D_B: 0.2500566840171814\n",
      "Epoch [20/200] Batch [300] Loss G: 2.523827075958252 Loss D_A: 0.1966574788093567 Loss D_B: 0.3221242129802704\n",
      "Epoch [20/200] Batch [400] Loss G: 2.5796101093292236 Loss D_A: 0.17054995894432068 Loss D_B: 0.23991695046424866\n",
      "Epoch [21/200] Batch [0] Loss G: 2.1960365772247314 Loss D_A: 0.22226247191429138 Loss D_B: 0.2734307646751404\n",
      "Epoch [21/200] Batch [100] Loss G: 2.4702858924865723 Loss D_A: 0.27246272563934326 Loss D_B: 0.18637658655643463\n",
      "Epoch [21/200] Batch [200] Loss G: 2.6423420906066895 Loss D_A: 0.1612142026424408 Loss D_B: 0.1734444797039032\n",
      "Epoch [21/200] Batch [300] Loss G: 3.033078670501709 Loss D_A: 0.1525290608406067 Loss D_B: 0.222005233168602\n",
      "Epoch [21/200] Batch [400] Loss G: 3.6137664318084717 Loss D_A: 1.4969162940979004 Loss D_B: 0.21812933683395386\n",
      "Epoch [22/200] Batch [0] Loss G: 2.4149088859558105 Loss D_A: 0.24822774529457092 Loss D_B: 0.28073573112487793\n",
      "Epoch [22/200] Batch [100] Loss G: 2.5518269538879395 Loss D_A: 0.250044047832489 Loss D_B: 0.1931094229221344\n",
      "Epoch [22/200] Batch [200] Loss G: 2.8967957496643066 Loss D_A: 0.2558680772781372 Loss D_B: 0.1773865520954132\n",
      "Epoch [22/200] Batch [300] Loss G: 2.673928737640381 Loss D_A: 0.23951831459999084 Loss D_B: 0.2162632942199707\n",
      "Epoch [22/200] Batch [400] Loss G: 2.512220621109009 Loss D_A: 0.17309218645095825 Loss D_B: 0.22968092560768127\n",
      "Epoch [23/200] Batch [0] Loss G: 2.2813854217529297 Loss D_A: 0.21085377037525177 Loss D_B: 0.1406170278787613\n",
      "Epoch [23/200] Batch [100] Loss G: 2.6335010528564453 Loss D_A: 0.11042934656143188 Loss D_B: 0.21962285041809082\n",
      "Epoch [23/200] Batch [200] Loss G: 2.8816702365875244 Loss D_A: 0.188856303691864 Loss D_B: 0.16313442587852478\n",
      "Epoch [23/200] Batch [300] Loss G: 2.6961655616760254 Loss D_A: 0.18322725594043732 Loss D_B: 0.16890545189380646\n",
      "Epoch [23/200] Batch [400] Loss G: 2.8754396438598633 Loss D_A: 0.148153156042099 Loss D_B: 0.2086714208126068\n",
      "Epoch [24/200] Batch [0] Loss G: 3.55429744720459 Loss D_A: 0.42461061477661133 Loss D_B: 0.21566814184188843\n",
      "Epoch [24/200] Batch [100] Loss G: 3.3404815196990967 Loss D_A: 0.3252997100353241 Loss D_B: 0.23770037293434143\n",
      "Epoch [24/200] Batch [200] Loss G: 2.4337944984436035 Loss D_A: 0.18979142606258392 Loss D_B: 0.1510031372308731\n",
      "Epoch [24/200] Batch [300] Loss G: 2.379239320755005 Loss D_A: 0.3873303532600403 Loss D_B: 0.20306077599525452\n",
      "Epoch [24/200] Batch [400] Loss G: 3.099295139312744 Loss D_A: 0.08380881696939468 Loss D_B: 0.2663356065750122\n",
      "Epoch [25/200] Batch [0] Loss G: 2.613823890686035 Loss D_A: 0.04426783323287964 Loss D_B: 0.20910626649856567\n",
      "Epoch [25/200] Batch [100] Loss G: 3.009887218475342 Loss D_A: 0.187858447432518 Loss D_B: 0.2019074261188507\n",
      "Epoch [25/200] Batch [200] Loss G: 2.329319477081299 Loss D_A: 0.14401178061962128 Loss D_B: 0.21972084045410156\n",
      "Epoch [25/200] Batch [300] Loss G: 2.286393642425537 Loss D_A: 0.23518066108226776 Loss D_B: 0.21345354616641998\n",
      "Epoch [25/200] Batch [400] Loss G: 2.449042320251465 Loss D_A: 0.24202373623847961 Loss D_B: 0.17266660928726196\n",
      "Epoch [26/200] Batch [0] Loss G: 2.572222948074341 Loss D_A: 0.24344564974308014 Loss D_B: 0.22682201862335205\n",
      "Epoch [26/200] Batch [100] Loss G: 2.2138564586639404 Loss D_A: 0.22934255003929138 Loss D_B: 0.2639361023902893\n",
      "Epoch [26/200] Batch [200] Loss G: 1.9471875429153442 Loss D_A: 0.21101035177707672 Loss D_B: 0.25538042187690735\n",
      "Epoch [26/200] Batch [300] Loss G: 2.534820079803467 Loss D_A: 0.2158598005771637 Loss D_B: 0.2515726685523987\n",
      "Epoch [26/200] Batch [400] Loss G: 2.333904504776001 Loss D_A: 0.1921398788690567 Loss D_B: 0.21208491921424866\n",
      "Epoch [27/200] Batch [0] Loss G: 2.310485363006592 Loss D_A: 0.30333584547042847 Loss D_B: 0.23977898061275482\n",
      "Epoch [27/200] Batch [100] Loss G: 3.1563687324523926 Loss D_A: 0.19625836610794067 Loss D_B: 0.22496497631072998\n",
      "Epoch [27/200] Batch [200] Loss G: 2.4128270149230957 Loss D_A: 0.12856151163578033 Loss D_B: 0.20992884039878845\n",
      "Epoch [27/200] Batch [300] Loss G: 2.56406307220459 Loss D_A: 0.11282696574926376 Loss D_B: 0.1745874285697937\n",
      "Epoch [27/200] Batch [400] Loss G: 2.8677453994750977 Loss D_A: 0.28416869044303894 Loss D_B: 0.3071124851703644\n",
      "Epoch [28/200] Batch [0] Loss G: 2.66574764251709 Loss D_A: 0.18580728769302368 Loss D_B: 0.1538335084915161\n",
      "Epoch [28/200] Batch [100] Loss G: 2.517756223678589 Loss D_A: 0.16533441841602325 Loss D_B: 0.19086387753486633\n",
      "Epoch [28/200] Batch [200] Loss G: 3.035736083984375 Loss D_A: 0.04685378074645996 Loss D_B: 0.24963390827178955\n",
      "Epoch [28/200] Batch [300] Loss G: 3.369184970855713 Loss D_A: 0.11531946063041687 Loss D_B: 0.23279708623886108\n",
      "Epoch [28/200] Batch [400] Loss G: 2.1506600379943848 Loss D_A: 0.3825733959674835 Loss D_B: 0.2541142404079437\n",
      "Epoch [29/200] Batch [0] Loss G: 2.5669965744018555 Loss D_A: 0.17720524966716766 Loss D_B: 0.2007388174533844\n",
      "Epoch [29/200] Batch [100] Loss G: 2.289616107940674 Loss D_A: 0.20436319708824158 Loss D_B: 0.21478700637817383\n",
      "Epoch [29/200] Batch [200] Loss G: 2.3980307579040527 Loss D_A: 0.22036047279834747 Loss D_B: 0.15433435142040253\n",
      "Epoch [29/200] Batch [300] Loss G: 2.492971658706665 Loss D_A: 0.1416868418455124 Loss D_B: 0.2606961131095886\n",
      "Epoch [29/200] Batch [400] Loss G: 2.5130701065063477 Loss D_A: 0.1099414974451065 Loss D_B: 0.3040722608566284\n",
      "Epoch [30/200] Batch [0] Loss G: 2.485424041748047 Loss D_A: 0.17112569510936737 Loss D_B: 0.23084476590156555\n",
      "Epoch [30/200] Batch [100] Loss G: 2.642416000366211 Loss D_A: 0.20108920335769653 Loss D_B: 0.1708919256925583\n",
      "Epoch [30/200] Batch [200] Loss G: 2.203153610229492 Loss D_A: 0.19311273097991943 Loss D_B: 0.24483783543109894\n",
      "Epoch [30/200] Batch [300] Loss G: 1.7840064764022827 Loss D_A: 0.11113911867141724 Loss D_B: 0.229803204536438\n",
      "Epoch [30/200] Batch [400] Loss G: 1.9125595092773438 Loss D_A: 0.5113760232925415 Loss D_B: 0.2716911733150482\n",
      "Epoch [31/200] Batch [0] Loss G: 2.2704670429229736 Loss D_A: 0.18268688023090363 Loss D_B: 0.19558113813400269\n",
      "Epoch [31/200] Batch [100] Loss G: 1.942777156829834 Loss D_A: 0.17296384274959564 Loss D_B: 0.2782062888145447\n",
      "Epoch [31/200] Batch [200] Loss G: 2.5056304931640625 Loss D_A: 0.1959143579006195 Loss D_B: 0.25712940096855164\n",
      "Epoch [31/200] Batch [300] Loss G: 2.630825996398926 Loss D_A: 0.09634614735841751 Loss D_B: 0.1896045207977295\n",
      "Epoch [31/200] Batch [400] Loss G: 2.6778464317321777 Loss D_A: 0.1264619678258896 Loss D_B: 0.2484869658946991\n",
      "Epoch [32/200] Batch [0] Loss G: 2.4436614513397217 Loss D_A: 0.262667179107666 Loss D_B: 0.23132547736167908\n",
      "Epoch [32/200] Batch [100] Loss G: 2.3598783016204834 Loss D_A: 0.21384787559509277 Loss D_B: 0.3336683511734009\n",
      "Epoch [32/200] Batch [200] Loss G: 2.7521567344665527 Loss D_A: 0.08754564076662064 Loss D_B: 0.2772342562675476\n",
      "Epoch [32/200] Batch [300] Loss G: 2.2209672927856445 Loss D_A: 0.16803301870822906 Loss D_B: 0.2282310128211975\n",
      "Epoch [32/200] Batch [400] Loss G: 2.765738010406494 Loss D_A: 0.05319508910179138 Loss D_B: 0.2076895534992218\n",
      "Epoch [33/200] Batch [0] Loss G: 2.153775930404663 Loss D_A: 0.14272814989089966 Loss D_B: 0.21549996733665466\n",
      "Epoch [33/200] Batch [100] Loss G: 2.368806838989258 Loss D_A: 0.15593719482421875 Loss D_B: 0.2517068386077881\n",
      "Epoch [33/200] Batch [200] Loss G: 2.8250412940979004 Loss D_A: 0.19871960580348969 Loss D_B: 0.17985671758651733\n",
      "Epoch [33/200] Batch [300] Loss G: 4.003785610198975 Loss D_A: 0.1438101828098297 Loss D_B: 0.2149389088153839\n",
      "Epoch [33/200] Batch [400] Loss G: 2.66630220413208 Loss D_A: 0.0712708830833435 Loss D_B: 0.24921266734600067\n",
      "Epoch [34/200] Batch [0] Loss G: 2.352600574493408 Loss D_A: 0.1443600058555603 Loss D_B: 0.22576263546943665\n",
      "Epoch [34/200] Batch [100] Loss G: 2.0694327354431152 Loss D_A: 0.14167536795139313 Loss D_B: 0.25486236810684204\n",
      "Epoch [34/200] Batch [200] Loss G: 2.5702974796295166 Loss D_A: 0.13190340995788574 Loss D_B: 0.21963953971862793\n",
      "Epoch [34/200] Batch [300] Loss G: 2.312013626098633 Loss D_A: 0.1192624568939209 Loss D_B: 0.23776045441627502\n",
      "Epoch [34/200] Batch [400] Loss G: 2.1836740970611572 Loss D_A: 0.31369251012802124 Loss D_B: 0.13463351130485535\n",
      "Epoch [35/200] Batch [0] Loss G: 2.820889711380005 Loss D_A: 0.02820483222603798 Loss D_B: 0.224724680185318\n",
      "Epoch [35/200] Batch [100] Loss G: 2.1990959644317627 Loss D_A: 0.25689512491226196 Loss D_B: 0.14947465062141418\n",
      "Epoch [35/200] Batch [200] Loss G: 1.7425743341445923 Loss D_A: 0.2548631429672241 Loss D_B: 0.33311575651168823\n",
      "Epoch [35/200] Batch [300] Loss G: 2.2294485569000244 Loss D_A: 0.24621181190013885 Loss D_B: 0.2854655981063843\n",
      "Epoch [35/200] Batch [400] Loss G: 2.1646604537963867 Loss D_A: 0.24799606204032898 Loss D_B: 0.21944251656532288\n",
      "Epoch [36/200] Batch [0] Loss G: 2.291003704071045 Loss D_A: 0.26018109917640686 Loss D_B: 0.20964688062667847\n",
      "Epoch [36/200] Batch [100] Loss G: 2.156994342803955 Loss D_A: 0.24704575538635254 Loss D_B: 0.24765482544898987\n",
      "Epoch [36/200] Batch [200] Loss G: 1.8104336261749268 Loss D_A: 0.25347384810447693 Loss D_B: 0.24155256152153015\n",
      "Epoch [36/200] Batch [300] Loss G: 2.012343406677246 Loss D_A: 0.24865922331809998 Loss D_B: 0.19760000705718994\n",
      "Epoch [36/200] Batch [400] Loss G: 2.3366503715515137 Loss D_A: 0.2500366270542145 Loss D_B: 0.18884390592575073\n",
      "Epoch [37/200] Batch [0] Loss G: 1.9272762537002563 Loss D_A: 0.2474135309457779 Loss D_B: 0.24614444375038147\n",
      "Epoch [37/200] Batch [100] Loss G: 1.950412631034851 Loss D_A: 0.25469011068344116 Loss D_B: 0.22410809993743896\n",
      "Epoch [37/200] Batch [200] Loss G: 2.111278533935547 Loss D_A: 0.25514352321624756 Loss D_B: 0.2013692855834961\n",
      "Epoch [37/200] Batch [300] Loss G: 2.2067372798919678 Loss D_A: 0.25775277614593506 Loss D_B: 0.22524455189704895\n",
      "Epoch [37/200] Batch [400] Loss G: 2.154694080352783 Loss D_A: 0.2424265742301941 Loss D_B: 0.21999666094779968\n",
      "Epoch [38/200] Batch [0] Loss G: 2.640902519226074 Loss D_A: 0.24940577149391174 Loss D_B: 0.2098141461610794\n",
      "Epoch [38/200] Batch [100] Loss G: 2.170827865600586 Loss D_A: 0.2552434206008911 Loss D_B: 0.17849531769752502\n",
      "Epoch [38/200] Batch [200] Loss G: 2.2676408290863037 Loss D_A: 0.2366468906402588 Loss D_B: 0.25133970379829407\n",
      "Epoch [38/200] Batch [300] Loss G: 2.019138813018799 Loss D_A: 0.2484251707792282 Loss D_B: 0.17818602919578552\n",
      "Epoch [38/200] Batch [400] Loss G: 2.1967484951019287 Loss D_A: 0.23064573109149933 Loss D_B: 0.2533009648323059\n",
      "Epoch [39/200] Batch [0] Loss G: 1.9753068685531616 Loss D_A: 0.24249276518821716 Loss D_B: 0.241929829120636\n",
      "Epoch [39/200] Batch [100] Loss G: 2.2357563972473145 Loss D_A: 0.302484393119812 Loss D_B: 0.2151329219341278\n",
      "Epoch [39/200] Batch [200] Loss G: 2.2881321907043457 Loss D_A: 0.09429063647985458 Loss D_B: 0.2398492395877838\n",
      "Epoch [39/200] Batch [300] Loss G: 1.9178345203399658 Loss D_A: 0.27134042978286743 Loss D_B: 0.26632389426231384\n",
      "Epoch [39/200] Batch [400] Loss G: 2.211061477661133 Loss D_A: 0.11940634250640869 Loss D_B: 0.2511352300643921\n",
      "Epoch [40/200] Batch [0] Loss G: 2.8280487060546875 Loss D_A: 0.10207522660493851 Loss D_B: 0.20182862877845764\n",
      "Epoch [40/200] Batch [100] Loss G: 2.1680965423583984 Loss D_A: 0.11899752914905548 Loss D_B: 0.22854314744472504\n",
      "Epoch [40/200] Batch [200] Loss G: 2.086649179458618 Loss D_A: 0.15676762163639069 Loss D_B: 0.18235668540000916\n",
      "Epoch [40/200] Batch [300] Loss G: 2.3274173736572266 Loss D_A: 0.14105957746505737 Loss D_B: 0.19002065062522888\n",
      "Epoch [40/200] Batch [400] Loss G: 2.462284803390503 Loss D_A: 0.11739600449800491 Loss D_B: 0.19508039951324463\n",
      "Epoch [41/200] Batch [0] Loss G: 2.126786708831787 Loss D_A: 0.22114205360412598 Loss D_B: 0.17301198840141296\n",
      "Epoch [41/200] Batch [100] Loss G: 2.071284770965576 Loss D_A: 0.2627369165420532 Loss D_B: 0.21886703372001648\n",
      "Epoch [41/200] Batch [200] Loss G: 2.6348772048950195 Loss D_A: 0.03628913313150406 Loss D_B: 0.22181329131126404\n",
      "Epoch [41/200] Batch [300] Loss G: 2.787041187286377 Loss D_A: 0.030726201832294464 Loss D_B: 0.23340843617916107\n",
      "Epoch [41/200] Batch [400] Loss G: 2.536376714706421 Loss D_A: 0.02644941955804825 Loss D_B: 0.23831850290298462\n",
      "Epoch [42/200] Batch [0] Loss G: 2.499405860900879 Loss D_A: 0.24209930002689362 Loss D_B: 0.19079896807670593\n",
      "Epoch [42/200] Batch [100] Loss G: 2.3689186573028564 Loss D_A: 0.25011903047561646 Loss D_B: 0.23544785380363464\n",
      "Epoch [42/200] Batch [200] Loss G: 2.021306037902832 Loss D_A: 0.21726259589195251 Loss D_B: 0.19346585869789124\n",
      "Epoch [42/200] Batch [300] Loss G: 1.9521563053131104 Loss D_A: 0.2348097264766693 Loss D_B: 0.17458988726139069\n",
      "Epoch [42/200] Batch [400] Loss G: 1.9533882141113281 Loss D_A: 0.20572742819786072 Loss D_B: 0.2386465221643448\n",
      "Epoch [43/200] Batch [0] Loss G: 2.416147470474243 Loss D_A: 0.1821030080318451 Loss D_B: 0.1689467430114746\n",
      "Epoch [43/200] Batch [100] Loss G: 2.8436968326568604 Loss D_A: 0.22733990848064423 Loss D_B: 0.3029942810535431\n",
      "Epoch [43/200] Batch [200] Loss G: 3.6370694637298584 Loss D_A: 0.2637217044830322 Loss D_B: 0.20823392271995544\n",
      "Epoch [43/200] Batch [300] Loss G: 2.5463438034057617 Loss D_A: 0.20200511813163757 Loss D_B: 0.2194736748933792\n",
      "Epoch [43/200] Batch [400] Loss G: 1.9668540954589844 Loss D_A: 0.1693744957447052 Loss D_B: 0.20744647085666656\n",
      "Epoch [44/200] Batch [0] Loss G: 1.8870794773101807 Loss D_A: 0.17015503346920013 Loss D_B: 0.19130447506904602\n",
      "Epoch [44/200] Batch [100] Loss G: 2.089815616607666 Loss D_A: 0.1856335997581482 Loss D_B: 0.27582383155822754\n",
      "Epoch [44/200] Batch [200] Loss G: 3.658668041229248 Loss D_A: 0.14938288927078247 Loss D_B: 0.2542106509208679\n",
      "Epoch [44/200] Batch [300] Loss G: 2.777864456176758 Loss D_A: 0.05802955478429794 Loss D_B: 0.1782422810792923\n",
      "Epoch [44/200] Batch [400] Loss G: 1.9034479856491089 Loss D_A: 0.32081249356269836 Loss D_B: 0.20586898922920227\n",
      "Epoch [45/200] Batch [0] Loss G: 2.189899444580078 Loss D_A: 0.1164863109588623 Loss D_B: 0.2253415286540985\n",
      "Epoch [45/200] Batch [100] Loss G: 3.0947439670562744 Loss D_A: 0.09697822481393814 Loss D_B: 0.2367480844259262\n",
      "Epoch [45/200] Batch [200] Loss G: 2.104888439178467 Loss D_A: 0.18023383617401123 Loss D_B: 0.21786795556545258\n",
      "Epoch [45/200] Batch [300] Loss G: 2.6532623767852783 Loss D_A: 0.13376902043819427 Loss D_B: 0.19344043731689453\n",
      "Epoch [45/200] Batch [400] Loss G: 3.376713752746582 Loss D_A: 0.17622685432434082 Loss D_B: 0.20163212716579437\n",
      "Epoch [46/200] Batch [0] Loss G: 2.170103073120117 Loss D_A: 0.2751777470111847 Loss D_B: 0.33538758754730225\n",
      "Epoch [46/200] Batch [100] Loss G: 2.2860007286071777 Loss D_A: 0.1589461863040924 Loss D_B: 0.21217796206474304\n",
      "Epoch [46/200] Batch [200] Loss G: 1.7092034816741943 Loss D_A: 0.18508154153823853 Loss D_B: 0.18181166052818298\n",
      "Epoch [46/200] Batch [300] Loss G: 2.52651309967041 Loss D_A: 0.2541114091873169 Loss D_B: 0.25182509422302246\n",
      "Epoch [46/200] Batch [400] Loss G: 2.090501070022583 Loss D_A: 0.6390326619148254 Loss D_B: 0.2705630660057068\n",
      "Epoch [47/200] Batch [0] Loss G: 2.2726752758026123 Loss D_A: 0.11499767005443573 Loss D_B: 0.2586834728717804\n",
      "Epoch [47/200] Batch [100] Loss G: 2.1278398036956787 Loss D_A: 0.26907405257225037 Loss D_B: 0.18185698986053467\n",
      "Epoch [47/200] Batch [200] Loss G: 2.4372193813323975 Loss D_A: 0.4075275659561157 Loss D_B: 0.22990334033966064\n",
      "Epoch [47/200] Batch [300] Loss G: 1.9606211185455322 Loss D_A: 0.16029848158359528 Loss D_B: 0.23972374200820923\n",
      "Epoch [47/200] Batch [400] Loss G: 2.3485817909240723 Loss D_A: 0.15318095684051514 Loss D_B: 0.20628179609775543\n",
      "Epoch [48/200] Batch [0] Loss G: 2.2900543212890625 Loss D_A: 0.06838666647672653 Loss D_B: 0.1872839331626892\n",
      "Epoch [48/200] Batch [100] Loss G: 2.242269992828369 Loss D_A: 0.18715979158878326 Loss D_B: 0.2027229517698288\n",
      "Epoch [48/200] Batch [200] Loss G: 1.77730393409729 Loss D_A: 0.1933773010969162 Loss D_B: 0.2153353989124298\n",
      "Epoch [48/200] Batch [300] Loss G: 2.2653493881225586 Loss D_A: 0.09349891543388367 Loss D_B: 0.2996501326560974\n",
      "Epoch [48/200] Batch [400] Loss G: 1.7439863681793213 Loss D_A: 0.30942603945732117 Loss D_B: 0.23212794959545135\n",
      "Epoch [49/200] Batch [0] Loss G: 2.7064733505249023 Loss D_A: 0.02871273085474968 Loss D_B: 0.326041579246521\n",
      "Epoch [49/200] Batch [100] Loss G: 2.0848917961120605 Loss D_A: 0.3638112246990204 Loss D_B: 0.20875665545463562\n",
      "Epoch [49/200] Batch [200] Loss G: 1.8734889030456543 Loss D_A: 0.24841928482055664 Loss D_B: 0.19639572501182556\n",
      "Epoch [49/200] Batch [300] Loss G: 1.8027750253677368 Loss D_A: 0.2644338607788086 Loss D_B: 0.26647019386291504\n",
      "Epoch [49/200] Batch [400] Loss G: 2.5729942321777344 Loss D_A: 0.038101986050605774 Loss D_B: 0.22889509797096252\n",
      "Epoch [50/200] Batch [0] Loss G: 2.6809706687927246 Loss D_A: 0.06954115629196167 Loss D_B: 0.19040946662425995\n",
      "Epoch [50/200] Batch [100] Loss G: 2.553708076477051 Loss D_A: 0.03233858570456505 Loss D_B: 0.20574358105659485\n",
      "Epoch [50/200] Batch [200] Loss G: 2.239701986312866 Loss D_A: 0.1249803900718689 Loss D_B: 0.19449764490127563\n",
      "Epoch [50/200] Batch [300] Loss G: 1.6117709875106812 Loss D_A: 0.1520194262266159 Loss D_B: 0.19813069701194763\n",
      "Epoch [50/200] Batch [400] Loss G: 3.04472017288208 Loss D_A: 0.07952117174863815 Loss D_B: 0.22386372089385986\n",
      "Epoch [51/200] Batch [0] Loss G: 2.173701763153076 Loss D_A: 0.21298763155937195 Loss D_B: 0.16722214221954346\n",
      "Epoch [51/200] Batch [100] Loss G: 2.0572214126586914 Loss D_A: 0.23325523734092712 Loss D_B: 0.23159630596637726\n",
      "Epoch [51/200] Batch [200] Loss G: 2.1714212894439697 Loss D_A: 0.05512828379869461 Loss D_B: 0.3024998903274536\n",
      "Epoch [51/200] Batch [300] Loss G: 2.128960132598877 Loss D_A: 0.1025111973285675 Loss D_B: 0.18261003494262695\n",
      "Epoch [51/200] Batch [400] Loss G: 2.2390570640563965 Loss D_A: 0.21090777218341827 Loss D_B: 0.2774789035320282\n",
      "Epoch [52/200] Batch [0] Loss G: 2.4216790199279785 Loss D_A: 0.06907247006893158 Loss D_B: 0.2102682888507843\n",
      "Epoch [52/200] Batch [100] Loss G: 1.8373980522155762 Loss D_A: 0.2536303997039795 Loss D_B: 0.23054245114326477\n",
      "Epoch [52/200] Batch [200] Loss G: 1.8041611909866333 Loss D_A: 0.26786768436431885 Loss D_B: 0.264538437128067\n",
      "Epoch [52/200] Batch [300] Loss G: 2.5907349586486816 Loss D_A: 0.22873258590698242 Loss D_B: 0.22762535512447357\n",
      "Epoch [52/200] Batch [400] Loss G: 2.2779412269592285 Loss D_A: 0.09450270980596542 Loss D_B: 0.18703369796276093\n",
      "Epoch [53/200] Batch [0] Loss G: 2.7286736965179443 Loss D_A: 0.026833459734916687 Loss D_B: 0.20407217741012573\n",
      "Epoch [53/200] Batch [100] Loss G: 1.9502702951431274 Loss D_A: 0.163502037525177 Loss D_B: 0.2518049478530884\n",
      "Epoch [53/200] Batch [200] Loss G: 2.135767936706543 Loss D_A: 0.15883901715278625 Loss D_B: 0.19295868277549744\n",
      "Epoch [53/200] Batch [300] Loss G: 2.02085542678833 Loss D_A: 0.21997401118278503 Loss D_B: 0.19214707612991333\n",
      "Epoch [53/200] Batch [400] Loss G: 3.1993165016174316 Loss D_A: 0.03609386831521988 Loss D_B: 0.1714654415845871\n",
      "Epoch [54/200] Batch [0] Loss G: 2.203047752380371 Loss D_A: 0.27579277753829956 Loss D_B: 0.25601309537887573\n",
      "Epoch [54/200] Batch [100] Loss G: 2.2980246543884277 Loss D_A: 0.07592467963695526 Loss D_B: 0.25924065709114075\n",
      "Epoch [54/200] Batch [200] Loss G: 2.1153955459594727 Loss D_A: 0.060111455619335175 Loss D_B: 0.24048081040382385\n",
      "Epoch [54/200] Batch [300] Loss G: 2.2074670791625977 Loss D_A: 0.15361198782920837 Loss D_B: 0.20322343707084656\n",
      "Epoch [54/200] Batch [400] Loss G: 2.1992435455322266 Loss D_A: 0.17774982750415802 Loss D_B: 0.2346794605255127\n",
      "Epoch [55/200] Batch [0] Loss G: 1.6852564811706543 Loss D_A: 0.29272595047950745 Loss D_B: 0.2521974742412567\n",
      "Epoch [55/200] Batch [100] Loss G: 2.5857958793640137 Loss D_A: 0.04096178710460663 Loss D_B: 0.17580029368400574\n",
      "Epoch [55/200] Batch [200] Loss G: 2.2086853981018066 Loss D_A: 0.07471625506877899 Loss D_B: 0.22886693477630615\n",
      "Epoch [55/200] Batch [300] Loss G: 2.1634879112243652 Loss D_A: 0.048314668238162994 Loss D_B: 0.23744183778762817\n",
      "Epoch [55/200] Batch [400] Loss G: 3.375040054321289 Loss D_A: 0.08169246464967728 Loss D_B: 0.19182321429252625\n",
      "Epoch [56/200] Batch [0] Loss G: 2.295560836791992 Loss D_A: 0.05579209327697754 Loss D_B: 0.18766359984874725\n",
      "Epoch [56/200] Batch [100] Loss G: 2.2385525703430176 Loss D_A: 0.04018835350871086 Loss D_B: 0.24486294388771057\n",
      "Epoch [56/200] Batch [200] Loss G: 2.288628101348877 Loss D_A: 0.2573409676551819 Loss D_B: 0.2897937297821045\n",
      "Epoch [56/200] Batch [300] Loss G: 2.5969491004943848 Loss D_A: 0.11681367456912994 Loss D_B: 0.23656848073005676\n",
      "Epoch [56/200] Batch [400] Loss G: 2.790633201599121 Loss D_A: 0.024399660527706146 Loss D_B: 0.2390984296798706\n",
      "Epoch [57/200] Batch [0] Loss G: 2.1306281089782715 Loss D_A: 0.1809777319431305 Loss D_B: 0.17899703979492188\n",
      "Epoch [57/200] Batch [100] Loss G: 1.92470383644104 Loss D_A: 0.22901783883571625 Loss D_B: 0.30705010890960693\n",
      "Epoch [57/200] Batch [200] Loss G: 2.1232829093933105 Loss D_A: 0.303109347820282 Loss D_B: 0.18940791487693787\n",
      "Epoch [57/200] Batch [300] Loss G: 1.9740376472473145 Loss D_A: 0.1469210982322693 Loss D_B: 0.20781557261943817\n",
      "Epoch [57/200] Batch [400] Loss G: 1.8704187870025635 Loss D_A: 0.20691919326782227 Loss D_B: 0.2204868495464325\n",
      "Epoch [58/200] Batch [0] Loss G: 1.7909473180770874 Loss D_A: 0.19221150875091553 Loss D_B: 0.1766808032989502\n",
      "Epoch [58/200] Batch [100] Loss G: 1.8009629249572754 Loss D_A: 0.2675049901008606 Loss D_B: 0.28347378969192505\n",
      "Epoch [58/200] Batch [200] Loss G: 2.532553195953369 Loss D_A: 0.09360872209072113 Loss D_B: 0.11816398799419403\n",
      "Epoch [58/200] Batch [300] Loss G: 3.096449851989746 Loss D_A: 0.04282519221305847 Loss D_B: 0.2060869038105011\n",
      "Epoch [58/200] Batch [400] Loss G: 2.0104808807373047 Loss D_A: 0.2217506319284439 Loss D_B: 0.26177671551704407\n",
      "Epoch [59/200] Batch [0] Loss G: 2.4824135303497314 Loss D_A: 0.11356458812952042 Loss D_B: 0.21111036837100983\n",
      "Epoch [59/200] Batch [100] Loss G: 1.7446881532669067 Loss D_A: 0.3055345416069031 Loss D_B: 0.21564912796020508\n",
      "Epoch [59/200] Batch [200] Loss G: 2.6992673873901367 Loss D_A: 0.06816554069519043 Loss D_B: 0.17883311212062836\n",
      "Epoch [59/200] Batch [300] Loss G: 2.0046958923339844 Loss D_A: 0.13085804879665375 Loss D_B: 0.24678575992584229\n",
      "Epoch [59/200] Batch [400] Loss G: 2.515120029449463 Loss D_A: 0.10726244747638702 Loss D_B: 0.17792782187461853\n",
      "Epoch [60/200] Batch [0] Loss G: 2.6089377403259277 Loss D_A: 0.03133899345993996 Loss D_B: 0.20066732168197632\n",
      "Epoch [60/200] Batch [100] Loss G: 2.5125503540039062 Loss D_A: 0.00940773356705904 Loss D_B: 0.1896929144859314\n",
      "Epoch [60/200] Batch [200] Loss G: 2.288315773010254 Loss D_A: 0.2696525454521179 Loss D_B: 0.2447194755077362\n",
      "Epoch [60/200] Batch [300] Loss G: 2.074024200439453 Loss D_A: 0.05146704614162445 Loss D_B: 0.2545313239097595\n",
      "Epoch [60/200] Batch [400] Loss G: 2.1594462394714355 Loss D_A: 0.17978626489639282 Loss D_B: 0.21579411625862122\n",
      "Epoch [61/200] Batch [0] Loss G: 2.551380157470703 Loss D_A: 0.023483440279960632 Loss D_B: 0.21285516023635864\n",
      "Epoch [61/200] Batch [100] Loss G: 2.458573818206787 Loss D_A: 0.031090736389160156 Loss D_B: 0.1672177016735077\n",
      "Epoch [61/200] Batch [200] Loss G: 2.245507001876831 Loss D_A: 0.024683697149157524 Loss D_B: 0.1900111585855484\n",
      "Epoch [61/200] Batch [300] Loss G: 2.6359832286834717 Loss D_A: 0.11820846050977707 Loss D_B: 0.19196414947509766\n",
      "Epoch [61/200] Batch [400] Loss G: 1.9324084520339966 Loss D_A: 0.1855950653553009 Loss D_B: 0.22651898860931396\n",
      "Epoch [62/200] Batch [0] Loss G: 2.0186777114868164 Loss D_A: 0.236814945936203 Loss D_B: 0.21986447274684906\n",
      "Epoch [62/200] Batch [100] Loss G: 2.670037269592285 Loss D_A: 0.05890774726867676 Loss D_B: 0.1422077715396881\n",
      "Epoch [62/200] Batch [200] Loss G: 2.3743886947631836 Loss D_A: 0.2471742182970047 Loss D_B: 0.1784079372882843\n",
      "Epoch [62/200] Batch [300] Loss G: 3.015331506729126 Loss D_A: 0.008559118956327438 Loss D_B: 0.18925681710243225\n",
      "Epoch [62/200] Batch [400] Loss G: 2.5476861000061035 Loss D_A: 0.02423020824790001 Loss D_B: 0.21994611620903015\n",
      "Epoch [63/200] Batch [0] Loss G: 3.0805325508117676 Loss D_A: 0.030497172847390175 Loss D_B: 0.21397583186626434\n",
      "Epoch [63/200] Batch [100] Loss G: 2.860027551651001 Loss D_A: 0.21538953483104706 Loss D_B: 0.2156466841697693\n",
      "Epoch [63/200] Batch [200] Loss G: 2.2318272590637207 Loss D_A: 0.12507957220077515 Loss D_B: 0.2676612138748169\n",
      "Epoch [63/200] Batch [300] Loss G: 1.930191993713379 Loss D_A: 0.22624480724334717 Loss D_B: 0.2285919189453125\n",
      "Epoch [63/200] Batch [400] Loss G: 2.2022316455841064 Loss D_A: 0.1950834095478058 Loss D_B: 0.20056095719337463\n",
      "Epoch [64/200] Batch [0] Loss G: 1.6765371561050415 Loss D_A: 0.24386700987815857 Loss D_B: 0.23234477639198303\n",
      "Epoch [64/200] Batch [100] Loss G: 2.3324217796325684 Loss D_A: 0.23834291100502014 Loss D_B: 0.21866509318351746\n",
      "Epoch [64/200] Batch [200] Loss G: 2.00557017326355 Loss D_A: 0.20157843828201294 Loss D_B: 0.1891830563545227\n",
      "Epoch [64/200] Batch [300] Loss G: 2.0694217681884766 Loss D_A: 0.3034197688102722 Loss D_B: 0.18318544328212738\n",
      "Epoch [64/200] Batch [400] Loss G: 1.9036760330200195 Loss D_A: 0.2812628149986267 Loss D_B: 0.25327980518341064\n",
      "Epoch [65/200] Batch [0] Loss G: 2.0516114234924316 Loss D_A: 0.20385098457336426 Loss D_B: 0.19326263666152954\n",
      "Epoch [65/200] Batch [100] Loss G: 2.6809651851654053 Loss D_A: 0.02393263950943947 Loss D_B: 0.20698750019073486\n",
      "Epoch [65/200] Batch [200] Loss G: 2.028911590576172 Loss D_A: 0.0858243852853775 Loss D_B: 0.21176588535308838\n",
      "Epoch [65/200] Batch [300] Loss G: 2.302032947540283 Loss D_A: 0.34024685621261597 Loss D_B: 0.22178833186626434\n",
      "Epoch [65/200] Batch [400] Loss G: 2.3566691875457764 Loss D_A: 0.061491064727306366 Loss D_B: 0.23645547032356262\n",
      "Epoch [66/200] Batch [0] Loss G: 2.85752272605896 Loss D_A: 0.024551335722208023 Loss D_B: 0.2417176365852356\n",
      "Epoch [66/200] Batch [100] Loss G: 2.067805528640747 Loss D_A: 0.14121028780937195 Loss D_B: 0.23923557996749878\n",
      "Epoch [66/200] Batch [200] Loss G: 2.342074394226074 Loss D_A: 0.0396849550306797 Loss D_B: 0.21209096908569336\n",
      "Epoch [66/200] Batch [300] Loss G: 1.90675687789917 Loss D_A: 0.21783673763275146 Loss D_B: 0.2993539571762085\n",
      "Epoch [66/200] Batch [400] Loss G: 2.1093831062316895 Loss D_A: 0.20314833521842957 Loss D_B: 0.2126559615135193\n",
      "Epoch [67/200] Batch [0] Loss G: 2.4953227043151855 Loss D_A: 0.013806955888867378 Loss D_B: 0.2244274914264679\n",
      "Epoch [67/200] Batch [100] Loss G: 3.0637006759643555 Loss D_A: 0.05954623222351074 Loss D_B: 0.2393520325422287\n",
      "Epoch [67/200] Batch [200] Loss G: 2.2357563972473145 Loss D_A: 0.06789665669202805 Loss D_B: 0.2767273783683777\n",
      "Epoch [67/200] Batch [300] Loss G: 2.140974760055542 Loss D_A: 0.2066100835800171 Loss D_B: 0.16857866942882538\n",
      "Epoch [67/200] Batch [400] Loss G: 2.447971820831299 Loss D_A: 0.022155165672302246 Loss D_B: 0.21332547068595886\n",
      "Epoch [68/200] Batch [0] Loss G: 1.7948060035705566 Loss D_A: 0.24236349761486053 Loss D_B: 0.2218332588672638\n",
      "Epoch [68/200] Batch [100] Loss G: 2.4806766510009766 Loss D_A: 0.1344762146472931 Loss D_B: 0.18687507510185242\n",
      "Epoch [68/200] Batch [200] Loss G: 1.6491636037826538 Loss D_A: 0.28277021646499634 Loss D_B: 0.17886772751808167\n",
      "Epoch [68/200] Batch [300] Loss G: 1.888364315032959 Loss D_A: 0.23125523328781128 Loss D_B: 0.21677610278129578\n",
      "Epoch [68/200] Batch [400] Loss G: 2.406365156173706 Loss D_A: 0.013212611898779869 Loss D_B: 0.21052362024784088\n",
      "Epoch [69/200] Batch [0] Loss G: 2.133937358856201 Loss D_A: 0.07279976457357407 Loss D_B: 0.24679458141326904\n",
      "Epoch [69/200] Batch [100] Loss G: 2.4773755073547363 Loss D_A: 0.05428432673215866 Loss D_B: 0.21863682568073273\n",
      "Epoch [69/200] Batch [200] Loss G: 2.3627853393554688 Loss D_A: 0.02865631692111492 Loss D_B: 0.246440127491951\n",
      "Epoch [69/200] Batch [300] Loss G: 2.331475257873535 Loss D_A: 0.022662831470370293 Loss D_B: 0.3235217332839966\n",
      "Epoch [69/200] Batch [400] Loss G: 1.8738231658935547 Loss D_A: 0.1558084487915039 Loss D_B: 0.24446654319763184\n",
      "Epoch [70/200] Batch [0] Loss G: 2.5582127571105957 Loss D_A: 0.01774848997592926 Loss D_B: 0.1897035539150238\n",
      "Epoch [70/200] Batch [100] Loss G: 2.39284610748291 Loss D_A: 0.00807888526469469 Loss D_B: 0.21037635207176208\n",
      "Epoch [70/200] Batch [200] Loss G: 2.43550443649292 Loss D_A: 0.14225366711616516 Loss D_B: 0.19142740964889526\n",
      "Epoch [70/200] Batch [300] Loss G: 2.2323479652404785 Loss D_A: 0.03611161187291145 Loss D_B: 0.20047739148139954\n",
      "Epoch [70/200] Batch [400] Loss G: 2.4977352619171143 Loss D_A: 0.004586385563015938 Loss D_B: 0.2438383847475052\n",
      "Epoch [71/200] Batch [0] Loss G: 2.517965793609619 Loss D_A: 0.007445014547556639 Loss D_B: 0.2540401220321655\n",
      "Epoch [71/200] Batch [100] Loss G: 2.389443874359131 Loss D_A: 0.02316243387758732 Loss D_B: 0.24386824667453766\n",
      "Epoch [71/200] Batch [200] Loss G: 2.7582972049713135 Loss D_A: 0.009603552520275116 Loss D_B: 0.21608594059944153\n",
      "Epoch [71/200] Batch [300] Loss G: 2.468334197998047 Loss D_A: 0.02799406833946705 Loss D_B: 0.1709943562746048\n",
      "Epoch [71/200] Batch [400] Loss G: 2.7976930141448975 Loss D_A: 0.010449578985571861 Loss D_B: 0.2384497970342636\n",
      "Epoch [72/200] Batch [0] Loss G: 2.860410690307617 Loss D_A: 0.03666272014379501 Loss D_B: 0.20206713676452637\n",
      "Epoch [72/200] Batch [100] Loss G: 2.4865756034851074 Loss D_A: 0.019634919241070747 Loss D_B: 0.264761745929718\n",
      "Epoch [72/200] Batch [200] Loss G: 1.889815330505371 Loss D_A: 0.22656981647014618 Loss D_B: 0.19876936078071594\n",
      "Epoch [72/200] Batch [300] Loss G: 2.4121079444885254 Loss D_A: 0.02915402688086033 Loss D_B: 0.23957563936710358\n",
      "Epoch [72/200] Batch [400] Loss G: 1.8571202754974365 Loss D_A: 0.24441246688365936 Loss D_B: 0.23643480241298676\n",
      "Epoch [73/200] Batch [0] Loss G: 1.807508945465088 Loss D_A: 0.242941215634346 Loss D_B: 0.20235253870487213\n",
      "Epoch [73/200] Batch [100] Loss G: 2.6769275665283203 Loss D_A: 0.09646669775247574 Loss D_B: 0.19857724010944366\n",
      "Epoch [73/200] Batch [200] Loss G: 2.6730737686157227 Loss D_A: 0.03433188050985336 Loss D_B: 0.14863133430480957\n",
      "Epoch [73/200] Batch [300] Loss G: 2.6714344024658203 Loss D_A: 0.018147697672247887 Loss D_B: 0.17514634132385254\n",
      "Epoch [73/200] Batch [400] Loss G: 2.1318371295928955 Loss D_A: 0.02568238414824009 Loss D_B: 0.22878655791282654\n",
      "Epoch [74/200] Batch [0] Loss G: 2.631645917892456 Loss D_A: 0.005660551600158215 Loss D_B: 0.15828797221183777\n",
      "Epoch [74/200] Batch [100] Loss G: 2.4199318885803223 Loss D_A: 0.017542006447911263 Loss D_B: 0.19472020864486694\n",
      "Epoch [74/200] Batch [200] Loss G: 3.080457925796509 Loss D_A: 0.02670147269964218 Loss D_B: 0.19438523054122925\n",
      "Epoch [74/200] Batch [300] Loss G: 2.9122207164764404 Loss D_A: 0.014379018917679787 Loss D_B: 0.21071173250675201\n",
      "Epoch [74/200] Batch [400] Loss G: 2.706258773803711 Loss D_A: 0.0026112752966582775 Loss D_B: 0.20115959644317627\n",
      "Epoch [75/200] Batch [0] Loss G: 2.6398227214813232 Loss D_A: 0.005384774878621101 Loss D_B: 0.1928640455007553\n",
      "Epoch [75/200] Batch [100] Loss G: 2.8237881660461426 Loss D_A: 0.007876257412135601 Loss D_B: 0.1707143783569336\n",
      "Epoch [75/200] Batch [200] Loss G: 2.625758171081543 Loss D_A: 0.0038379705511033535 Loss D_B: 0.20156434178352356\n",
      "Epoch [75/200] Batch [300] Loss G: 2.1880884170532227 Loss D_A: 0.15182435512542725 Loss D_B: 0.1670025885105133\n",
      "Epoch [75/200] Batch [400] Loss G: 2.756680488586426 Loss D_A: 0.006353588774800301 Loss D_B: 0.2920747995376587\n",
      "Epoch [76/200] Batch [0] Loss G: 2.634718179702759 Loss D_A: 0.008718457072973251 Loss D_B: 0.22280976176261902\n",
      "Epoch [76/200] Batch [100] Loss G: 1.904342532157898 Loss D_A: 0.23748213052749634 Loss D_B: 0.19704711437225342\n",
      "Epoch [76/200] Batch [200] Loss G: 1.8306643962860107 Loss D_A: 0.24470274150371552 Loss D_B: 0.1935691386461258\n",
      "Epoch [76/200] Batch [300] Loss G: 1.6522010564804077 Loss D_A: 0.17066025733947754 Loss D_B: 0.28062179684638977\n",
      "Epoch [76/200] Batch [400] Loss G: 1.6679563522338867 Loss D_A: 0.2612718641757965 Loss D_B: 0.20509859919548035\n",
      "Epoch [77/200] Batch [0] Loss G: 1.7557026147842407 Loss D_A: 0.255141943693161 Loss D_B: 0.24125255644321442\n",
      "Epoch [77/200] Batch [100] Loss G: 1.6187045574188232 Loss D_A: 0.25795066356658936 Loss D_B: 0.21362680196762085\n",
      "Epoch [77/200] Batch [200] Loss G: 2.9203057289123535 Loss D_A: 0.017947185784578323 Loss D_B: 0.18380093574523926\n",
      "Epoch [77/200] Batch [300] Loss G: 1.8791362047195435 Loss D_A: 0.2280125916004181 Loss D_B: 0.23430678248405457\n",
      "Epoch [77/200] Batch [400] Loss G: 1.8513219356536865 Loss D_A: 0.2282225489616394 Loss D_B: 0.22973671555519104\n",
      "Epoch [78/200] Batch [0] Loss G: 1.990614414215088 Loss D_A: 0.20593500137329102 Loss D_B: 0.18224593997001648\n",
      "Epoch [78/200] Batch [100] Loss G: 2.2529993057250977 Loss D_A: 0.1981794536113739 Loss D_B: 0.1542939990758896\n",
      "Epoch [78/200] Batch [200] Loss G: 1.6638004779815674 Loss D_A: 0.2436400055885315 Loss D_B: 0.20139622688293457\n",
      "Epoch [78/200] Batch [300] Loss G: 2.6192946434020996 Loss D_A: 0.013112913817167282 Loss D_B: 0.18292205035686493\n",
      "Epoch [78/200] Batch [400] Loss G: 1.8990012407302856 Loss D_A: 0.248187854886055 Loss D_B: 0.20550782978534698\n",
      "Epoch [79/200] Batch [0] Loss G: 2.4912071228027344 Loss D_A: 0.058891184628009796 Loss D_B: 0.23067766427993774\n",
      "Epoch [79/200] Batch [100] Loss G: 1.6992664337158203 Loss D_A: 0.3085139989852905 Loss D_B: 0.21617820858955383\n",
      "Epoch [79/200] Batch [200] Loss G: 2.051888942718506 Loss D_A: 0.205337256193161 Loss D_B: 0.2049243450164795\n",
      "Epoch [79/200] Batch [300] Loss G: 1.6835060119628906 Loss D_A: 0.2512253522872925 Loss D_B: 0.22137615084648132\n",
      "Epoch [79/200] Batch [400] Loss G: 1.841667652130127 Loss D_A: 0.20868945121765137 Loss D_B: 0.21970134973526\n",
      "Epoch [80/200] Batch [0] Loss G: 1.783707857131958 Loss D_A: 0.21249864995479584 Loss D_B: 0.21883535385131836\n",
      "Epoch [80/200] Batch [100] Loss G: 3.0467731952667236 Loss D_A: 0.008087054826319218 Loss D_B: 0.1843947470188141\n",
      "Epoch [80/200] Batch [200] Loss G: 2.0184102058410645 Loss D_A: 0.19795575737953186 Loss D_B: 0.20923849940299988\n",
      "Epoch [80/200] Batch [300] Loss G: 2.713714599609375 Loss D_A: 0.007630554959177971 Loss D_B: 0.19830983877182007\n",
      "Epoch [80/200] Batch [400] Loss G: 2.889378786087036 Loss D_A: 0.0129602225497365 Loss D_B: 0.12512052059173584\n",
      "Epoch [81/200] Batch [0] Loss G: 2.4372780323028564 Loss D_A: 0.015843845903873444 Loss D_B: 0.18853473663330078\n",
      "Epoch [81/200] Batch [100] Loss G: 1.9221848249435425 Loss D_A: 0.18326404690742493 Loss D_B: 0.18914088606834412\n",
      "Epoch [81/200] Batch [200] Loss G: 2.83198618888855 Loss D_A: 0.016766423359513283 Loss D_B: 0.17647908627986908\n",
      "Epoch [81/200] Batch [300] Loss G: 1.9435149431228638 Loss D_A: 0.053295571357011795 Loss D_B: 0.21980232000350952\n",
      "Epoch [81/200] Batch [400] Loss G: 1.7779314517974854 Loss D_A: 0.152388796210289 Loss D_B: 0.19588309526443481\n",
      "Epoch [82/200] Batch [0] Loss G: 2.5187087059020996 Loss D_A: 0.013299520127475262 Loss D_B: 0.24874034523963928\n",
      "Epoch [82/200] Batch [100] Loss G: 2.7000157833099365 Loss D_A: 0.013401740230619907 Loss D_B: 0.21393901109695435\n",
      "Epoch [82/200] Batch [200] Loss G: 2.046907424926758 Loss D_A: 0.21584957838058472 Loss D_B: 0.19435229897499084\n",
      "Epoch [82/200] Batch [300] Loss G: 2.108358860015869 Loss D_A: 0.2572444677352905 Loss D_B: 0.19422930479049683\n",
      "Epoch [82/200] Batch [400] Loss G: 1.8077924251556396 Loss D_A: 0.1275143325328827 Loss D_B: 0.1826382726430893\n",
      "Epoch [83/200] Batch [0] Loss G: 2.699070692062378 Loss D_A: 0.010540563613176346 Loss D_B: 0.24053770303726196\n",
      "Epoch [83/200] Batch [100] Loss G: 2.90771746635437 Loss D_A: 0.01827852427959442 Loss D_B: 0.18394887447357178\n",
      "Epoch [83/200] Batch [200] Loss G: 2.839576244354248 Loss D_A: 0.008928687311708927 Loss D_B: 0.21114876866340637\n",
      "Epoch [83/200] Batch [300] Loss G: 2.8638386726379395 Loss D_A: 0.021560272201895714 Loss D_B: 0.1975361704826355\n",
      "Epoch [83/200] Batch [400] Loss G: 2.7559940814971924 Loss D_A: 0.0039566741324961185 Loss D_B: 0.23138311505317688\n",
      "Epoch [84/200] Batch [0] Loss G: 2.4281527996063232 Loss D_A: 0.009076088666915894 Loss D_B: 0.18520936369895935\n",
      "Epoch [84/200] Batch [100] Loss G: 2.369598627090454 Loss D_A: 0.010272175073623657 Loss D_B: 0.1895383596420288\n",
      "Epoch [84/200] Batch [200] Loss G: 2.3672730922698975 Loss D_A: 0.0028463639318943024 Loss D_B: 0.24373281002044678\n",
      "Epoch [84/200] Batch [300] Loss G: 2.6816837787628174 Loss D_A: 0.0027743405662477016 Loss D_B: 0.21093085408210754\n",
      "Epoch [84/200] Batch [400] Loss G: 2.610874652862549 Loss D_A: 0.005918706767261028 Loss D_B: 0.21118661761283875\n",
      "Epoch [85/200] Batch [0] Loss G: 2.278909683227539 Loss D_A: 0.01179155707359314 Loss D_B: 0.1747521013021469\n",
      "Epoch [85/200] Batch [100] Loss G: 2.4588632583618164 Loss D_A: 0.00567992078140378 Loss D_B: 0.19349360466003418\n",
      "Epoch [85/200] Batch [200] Loss G: 2.524268627166748 Loss D_A: 0.0025557614862918854 Loss D_B: 0.23772503435611725\n",
      "Epoch [85/200] Batch [300] Loss G: 1.8905394077301025 Loss D_A: 0.2611262798309326 Loss D_B: 0.11623866856098175\n",
      "Epoch [85/200] Batch [400] Loss G: 1.8566668033599854 Loss D_A: 0.1951051652431488 Loss D_B: 0.1944332718849182\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "# Setting up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Directories for dataset and saving checkpoints\n",
    "dataset_path = \"./flowers\"\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8  # Increased batch size for more stable training if GPU allows\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 200\n",
    "image_size = 256\n",
    "\n",
    "# Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset_A = datasets.ImageFolder(root=os.path.join(dataset_path, 'A'), transform=transform)\n",
    "dataset_B = datasets.ImageFolder(root=os.path.join(dataset_path, 'B'), transform=transform)\n",
    "loader_A = DataLoader(dataset_A, batch_size=batch_size, shuffle=True)\n",
    "loader_B = DataLoader(dataset_B, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Downsampling\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Residual Blocks\n",
    "            *[ResidualBlock(256) for _ in range(12)],  # Increased number of residual blocks for a larger model\n",
    "            # Upsampling\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Initialize models\n",
    "G_A2B = Generator().to(device)\n",
    "G_B2A = Generator().to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D_A = optim.Adam(D_A.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D_B = optim.Adam(D_B.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "\n",
    "# Training Loop\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_A, real_B) in enumerate(zip(loader_A, loader_B)):\n",
    "        real_A = real_A[0].to(device)\n",
    "        real_B = real_B[0].to(device)\n",
    "\n",
    "        # Train Generators\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN Loss A2B\n",
    "        fake_B = G_A2B(real_A)\n",
    "        pred_fake = D_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "        # GAN Loss B2A\n",
    "        fake_A = G_B2A(real_B)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "        # Cycle Loss\n",
    "        recovered_A = G_B2A(fake_B)\n",
    "        recovered_B = G_A2B(fake_A)\n",
    "        loss_cycle_A = criterion_cycle(recovered_A, real_A)\n",
    "        loss_cycle_B = criterion_cycle(recovered_B, real_B)\n",
    "\n",
    "        # Total Generator Loss\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + 10.0 * (loss_cycle_A + loss_cycle_B)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminator A\n",
    "        optimizer_D_A.zero_grad()\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # Train Discriminator B\n",
    "        optimizer_D_B.zero_grad()\n",
    "        pred_real = D_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "        pred_fake = D_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}] Loss G: {loss_G.item()} Loss D_A: {loss_D_A.item()} Loss D_B: {loss_D_B.item()}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 10 == 0 or loss_G.item() < best_loss:\n",
    "        torch.save(G_A2B.state_dict(), f\"{checkpoint_dir}/G_A2B_{epoch+1}.pth\")\n",
    "        torch.save(G_B2A.state_dict(), f\"{checkpoint_dir}/G_B2A_{epoch+1}.pth\")\n",
    "        best_loss = loss_G.item()\n",
    "\n",
    "print(\"Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155989/1337411875.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G_A2B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'G_A2B_1.pth')))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Setting up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Directories for inference\n",
    "inference_dir = \"./inference\"\n",
    "inference_output_dir = \"./inference_colour\"\n",
    "os.makedirs(inference_output_dir, exist_ok=True)\n",
    "\n",
    "# Load pre-trained generator\n",
    "G_A2B = Generator().to(device)\n",
    "G_A2B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'G_A2B_1.pth')))\n",
    "G_A2B.eval()\n",
    "\n",
    "# Inference Transformation\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Inference Loop\n",
    "for img_name in os.listdir(inference_dir):\n",
    "    img_path = os.path.join(inference_dir, img_name)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    input_tensor = inference_transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        colored_image = G_A2B(input_tensor).cpu().squeeze(0)\n",
    "    \n",
    "    # Denormalize and save image\n",
    "    colored_image = transforms.ToPILImage()(colored_image * 0.5 + 0.5)  # Denormalize\n",
    "    colored_image.save(os.path.join(inference_output_dir, img_name))\n",
    "\n",
    "print(\"Inference Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
